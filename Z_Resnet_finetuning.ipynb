{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dda726c6-1243-46f5-843e-c829b695ff08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29ef893f-1302-4ee8-b1e5-3da70f355383",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torchvision import models\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3269970-6c45-4f23-a484-2b2e798f4dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from augmentation import augment_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89c2058b-9240-4633-a74d-ee1a6110cc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load('data/3x3_data_nparray.npy')\n",
    "y = np.load('data/3x3_labels_nparray.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a2d1dee-c736-46bd-a692-1a92943437a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_samples = X.shape[0]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=24, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76fb4bb0-a5a3-4e1d-8a49-e76d099eb052",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tensor = torch.from_numpy(X_train).float()\n",
    "y_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "y_tensor = torch.argmax(y_tensor, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9529ca4d-d92b-4f5f-a613-9878b401f3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = torch.bincount(y_tensor) # Creates a vector with counts of data for each class\n",
    "minority_classes = torch.where(class_counts < 1000)[0]\n",
    "\n",
    "minority_indices = torch.cat([torch.where(y_tensor == cls)[0] for cls in minority_classes])\n",
    "\n",
    "minority_data = X_tensor[minority_indices]\n",
    "minority_labels = y_tensor[minority_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7acde74-14ad-4afe-ae7e-9c7bfa06d4c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New dataset size: torch.Size([50107, 18, 3, 3]), torch.Size([50107])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matte\\AppData\\Local\\Temp\\ipykernel_5460\\1065669278.py:12: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:281.)\n",
      "  augmented_data = torch.tensor(augmented_data, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "augmented_data = []\n",
    "augmented_labels = []\n",
    "\n",
    "for i, image in enumerate(minority_data):\n",
    "    img1, img2, img3, img4, img5 = augment_image(image.numpy())  # Convert tensor to numpy for processing\n",
    "    \n",
    "    augmented_data.extend([img1, img2, img3, img4, img5])\n",
    "    \n",
    "    augmented_labels.extend([minority_labels[i].item()] * 5)# Append the same label\n",
    "\n",
    "# Convert back to tensors\n",
    "augmented_data = torch.tensor(augmented_data, dtype=torch.float32)\n",
    "augmented_labels = torch.tensor(augmented_labels, dtype=torch.long)\n",
    "\n",
    "# Combine augmented data with original dataset\n",
    "X_augmented = torch.cat((X_tensor, augmented_data), dim=0).permute(0,3,1,2)\n",
    "y_augmented = torch.cat((y_tensor, augmented_labels), dim=0)\n",
    "\n",
    "print(f\"New dataset size: {X_augmented.shape}, {y_augmented.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f9deaa0-0268-48d4-ac11-76a0037f445e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = y_augmented.max().item() + 1  # Determine the number of classes (C)\n",
    "y_tensor_one_hot = torch.nn.functional.one_hot(y_augmented, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18fbdd89-1018-46d2-afd1-51ec8bf543a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(X_augmented, y_tensor_one_hot)\n",
    "\n",
    "# Crea un DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b97b624-2666-4af6-b202-35e903159922",
   "metadata": {},
   "outputs": [],
   "source": [
    "from resnet_baseline import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc946182-f58d-4418-98a1-d79275cbf792",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matte\\.conda\\envs\\dseo\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\matte\\.conda\\envs\\dseo\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.8924\n"
     ]
    }
   ],
   "source": [
    "model = model(dataloader, 18, 19, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ea5d89-3e77-42f9-9205-ec3eb84b7751",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "X_torch = torch.from_numpy(X_test).permute(0,3,1,2)\n",
    "\n",
    "y_pred = model(X_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e3956f-dc01-4beb-895f-acb28a2c4570",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_np = y_pred.detach().cpu().numpy()\n",
    "y_pred_labels = np.argmax(y_pred_np, axis=1)\n",
    "\n",
    "y_test_np = np.argmax(y_test, axis=1)\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(y_test_np, y_pred_labels)\n",
    "plt.show()\n",
    "\n",
    "acc = accuracy_score(y_test_np, y_pred_labels)\n",
    "print(\"Accuracy ResNet18 fine-tuned with Focal Loss and data augmentation: \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53eb7133-b831-48cb-a35f-f88f1f269c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "band_idx = 1\n",
    "\n",
    "# Select pixel location (e.g., center pixel at (1,1))\n",
    "pixel_x, pixel_y = 1, 1\n",
    "\n",
    "# Extract values for the chosen band and pixel across all samples\n",
    "pixel_values = X_augmented[:, band_idx, pixel_x, pixel_y]\n",
    "\n",
    "# Plot distributions for each class\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "#y = np.argmax(y, axis=1)\n",
    "\n",
    "# Use a color palette for visibility\n",
    "palette = sns.color_palette(\"tab20\", 19)  # 19 distinct colors\n",
    "\n",
    "for class_id in range(19):\n",
    "    class_values = pixel_values[y_augmented == class_id]  # Filter values for the current class\n",
    "    sns.kdeplot(class_values, label=f\"Class {class_id}\", color=palette[class_id], alpha=0.7)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(f\"Pixel Value (Band {band_idx}, Pixel {pixel_x},{pixel_y})\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Distribution of Pixel Values by Class\")\n",
    "plt.legend(title=\"Class\", bbox_to_anchor=(1.05, 1), loc='upper left')  # Legend outside the plot\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cfb22a-1259-4e65-9291-b9f6d3041936",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
