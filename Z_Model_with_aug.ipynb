{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "097bba13-1813-4d65-8e32-2b410e7e003d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9a04f7c-b5df-41d4-95ba-3ce59fba5a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torchvision import models\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3536faf-9225-478d-a704-6bf403d0a439",
   "metadata": {},
   "outputs": [],
   "source": [
    "from augmentation import augment_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21daf1a5-91fd-4126-a95a-86bcbae61786",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load('data/3x3_data_nparray.npy')\n",
    "y = np.load('data/3x3_labels_nparray.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e31235f9-4efb-4a60-b195-461b7487e642",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_samples = X.shape[0]\n",
    "#X_num = X_augmented.numpy()\n",
    "#y_num = y_tensor_one_hot.numpy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=24, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e5bba63-b823-4155-a8bc-31c7864e4677",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tensor = torch.from_numpy(X_train)\n",
    "y_tensor = torch.from_numpy(y_train)\n",
    "y_tensor = torch.argmax(y_tensor, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0683c9f5-bc59-4ea5-8a54-1b30fecedca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = torch.bincount(y_tensor)\n",
    "minority_classes = torch.where(class_counts < 1000)[0]\n",
    "\n",
    "minority_indices = torch.cat([torch.where(y_tensor == cls)[0] for cls in minority_classes])\n",
    "\n",
    "minority_data = X_tensor[minority_indices]\n",
    "minority_labels = y_tensor[minority_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "036774b9-52cf-4966-bc5c-e9c59eb846fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New dataset size: torch.Size([50107, 18, 3, 3]), torch.Size([50107])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matte\\AppData\\Local\\Temp\\ipykernel_18204\\1065669278.py:12: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:281.)\n",
      "  augmented_data = torch.tensor(augmented_data, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "augmented_data = []\n",
    "augmented_labels = []\n",
    "\n",
    "for i, image in enumerate(minority_data):\n",
    "    img1, img2, img3, img4, img5 = augment_image(image.numpy())  # Convert tensor to numpy for processing\n",
    "    \n",
    "    augmented_data.extend([img1, img2, img3, img4, img5])\n",
    "    \n",
    "    augmented_labels.extend([minority_labels[i].item()] * 5)# Append the same label\n",
    "\n",
    "# Convert back to tensors\n",
    "augmented_data = torch.tensor(augmented_data, dtype=torch.float32)\n",
    "augmented_labels = torch.tensor(augmented_labels, dtype=torch.long)\n",
    "\n",
    "# Combine augmented data with original dataset\n",
    "X_augmented = torch.cat((X_tensor, augmented_data), dim=0).permute(0,3,1,2)\n",
    "y_augmented = torch.cat((y_tensor, augmented_labels), dim=0)\n",
    "\n",
    "print(f\"New dataset size: {X_augmented.shape}, {y_augmented.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "879642f2-6049-4750-8e61-8d19503bd044",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_augmented = torch.nn.functional.one_hot(y_augmented, num_classes=19)\n",
    "dataset = TensorDataset(X_augmented, y_augmented)\n",
    "\n",
    "# Crea un DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bbe913a-e247-44d2-8ad7-f56bf2ebb59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from resnet_baseline import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fc110d-cef9-4060-bf5e-6b662c237835",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matte\\.conda\\envs\\dseo\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\matte\\.conda\\envs\\dseo\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.0709\n"
     ]
    }
   ],
   "source": [
    "model = model(dataloader, 18, 19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a58105-9349-48f4-baa1-b3040ea9bd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "X_torch = torch.from_numpy(X_test).permute(0,3,1,2)\n",
    "\n",
    "y_pred = model(X_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10c2f63-a0cd-4003-8506-31a7e408c36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_np = y_pred.detach().cpu().numpy()\n",
    "y_pred_labels = np.argmax(y_pred_np, axis=1)\n",
    "\n",
    "y_test_np = np.argmax(y_test, axis=1)\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(y_test_np, y_pred_labels)\n",
    "plt.show()\n",
    "\n",
    "acc = accuracy_score(y_test_np, y_pred_labels)\n",
    "print(\"Accuracy ResNet18 with Focal Loss: \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095fc8c7-b16f-4536-b2dd-11e449b04e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.argmax(y_augmented, axis=1), bins = 19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3260d9dc-eaf9-4749-afc2-ee3093c0edbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
